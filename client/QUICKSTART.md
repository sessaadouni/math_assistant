# ğŸš€ DÃ©marrage rapide - Math RAG (Version modulaire)

## âœ… PrÃ©requis

- Python 3.11+
- Node.js 18+
- Ollama installÃ© avec les modÃ¨les :
  - `deepseek-v3.1:671b-cloud` (gÃ©nÃ©ration)
  - `mxbai-embed-large:latest` (embeddings)

## ğŸ“¦ Installation

### 1. Backend Python
```bash
# Retour au dossier racine
cd /home/se/test_ollama_rag

# Activer l'environnement virtuel (si nÃ©cessaire)
# python -m venv venv
# source venv/bin/activate

# Installer les dÃ©pendances (si pas dÃ©jÃ  fait)
pip install fastapi uvicorn langchain langchain-ollama langchain-chroma chromadb sse-starlette pymupdf pypdf python-multipart
```

### 2. Frontend Next.js
```bash
cd client

# Installer les dÃ©pendances
npm install

# DÃ©pendances principales dÃ©jÃ  dans package.json :
# - next, react, react-dom
# - @tanstack/react-query (ajoutÃ© manuellement par l'utilisateur)
# - framer-motion
# - react-markdown, remark-math, remark-gfm, rehype-katex
# - katex
# - tailwindcss
```

## ğŸ¬ Lancement

### Terminal 1 : Backend FastAPI
```bash
cd /home/se/test_ollama_rag
python server.py
```

Attendre le message :
```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### Terminal 2 : Frontend Next.js
```bash
cd /home/se/test_ollama_rag/client
npm run dev
```

Attendre le message :
```
âœ“ Ready in 2.3s
âœ Local:   http://localhost:3000
```

## ğŸŒ AccÃ¨s

- **Frontend** : http://localhost:3000
- **Backend API** : http://localhost:8000
- **Health check** : http://localhost:8000/health

## ğŸ§ª Test rapide

### 1. VÃ©rifier le backend
```bash
curl http://localhost:8000/health
```

RÃ©ponse attendue :
```json
{"ok":true,"model":"deepseek-v3.1:671b-cloud"}
```

### 2. Ouvrir le frontend
```
http://localhost:3000
```

Vous devriez voir :
- âœ… Header avec "Backend OK" (point vert)
- âœ… Onglets de navigation (Chat, Fiche, etc.)
- âœ… Panel Chat par dÃ©faut

### 3. Tester le streaming
1. Aller dans l'onglet **Chat** ğŸ’¬
2. Saisir : "Comment dÃ©montrer qu'une suite converge ?"
3. Cliquer sur "Poser la question"
4. Observer le streaming en temps rÃ©el

## ğŸ“ Structure du projet (nouvelle version)

```
/home/se/test_ollama_rag/
â”œâ”€â”€ server.py                  # ğŸ”´ Backend FastAPI
â”œâ”€â”€ math_course_rag_v2.py      # ğŸ”´ RAG avec ChromaDB
â”œâ”€â”€ prompts.py                 # ğŸ”´ Prompts pour le LLM
â”œâ”€â”€ livre_2011.pdf             # ğŸ“– PDF du cours (1268 pages)
â”œâ”€â”€ chroma_db_math_v2/         # ğŸ’¾ Vector store (2994 chunks)
â”‚
â””â”€â”€ client/                    # ğŸŸ¢ Frontend Next.js
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ app/
    â”‚   â”‚   â”œâ”€â”€ layout.tsx     # Layout avec Providers
    â”‚   â”‚   â”œâ”€â”€ page.tsx       # Page principale
    â”‚   â”‚   â””â”€â”€ MathRagApp.tsx # Composant principal (55 lignes)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ Providers.tsx  # TanStack Query Provider
    â”‚   â”‚   â”œâ”€â”€ ui/            # 7 composants UI rÃ©utilisables
    â”‚   â”‚   â””â”€â”€ features/      # 9 composants mÃ©tier (panels)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ hooks/             # 3 custom hooks
    â”‚   â”œâ”€â”€ lib/               # 4 utilitaires
    â”‚   â”œâ”€â”€ types/             # DÃ©finitions TypeScript
    â”‚   â””â”€â”€ styles/            # CSS markdown + math
    â”‚
    â”œâ”€â”€ tsconfig.json          # Config TypeScript (@/ alias)
    â”œâ”€â”€ package.json
    â”‚
    â”œâ”€â”€ ARCHITECTURE.md        # ğŸ“˜ Documentation architecture
    â”œâ”€â”€ MIGRATION.md           # ğŸ“— Guide de migration
    â””â”€â”€ DEMARRAGE.md           # ğŸ“™ Guide dÃ©marrage (ancien)
```

## ğŸ¯ FonctionnalitÃ©s disponibles

### 7 Panels interactifs

1. **ğŸ’¬ Chat** - Q&A avec filtres (doc type, chapitre, k)
2. **ğŸ“ Fiche** - GÃ©nÃ©ration de fiches d'exercices (thÃ¨me, niveau)
3. **âœ… RÃ©vision** - Correction de fiches complÃ©tÃ©es
4. **ğŸ§® Formule** - Recherche de formules mathÃ©matiques
5. **ğŸ“‹ Examen** - GÃ©nÃ©ration d'examens (multi-chapitres, durÃ©e)
6. **ğŸ“– Cours** - RÃ©sumÃ©s de cours (notion, niveau de dÃ©tail)
7. **ğŸ¯ Note** - Ã‰valuation de travaux d'Ã©lÃ¨ves

### CaractÃ©ristiques techniques

- âœ… **Streaming SSE** - RÃ©ponses en temps rÃ©el
- âœ… **TanStack Query** - Gestion cache et API
- âœ… **Persistance** - Formulaires sauvegardÃ©s dans localStorage
- âœ… **Health check** - VÃ©rification backend toutes les 30s
- âœ… **Markdown + KaTeX** - Rendu formules mathÃ©matiques
- âœ… **Animations** - Framer Motion pour les transitions
- âœ… **Dark UI** - Interface sombre moderne
- âœ… **Responsive** - Design adaptatif

## ğŸ› Debugging

### Backend ne dÃ©marre pas
```bash
# VÃ©rifier les ports
lsof -i :8000

# VÃ©rifier Ollama
ollama list

# Lancer Ollama si nÃ©cessaire
ollama serve
```

### Frontend ne dÃ©marre pas
```bash
# Nettoyer et rÃ©installer
cd client
rm -rf .next node_modules package-lock.json
npm install
npm run dev
```

### Point rouge "Backend hors ligne"
1. VÃ©rifier que `server.py` tourne bien
2. Tester : `curl http://localhost:8000/health`
3. VÃ©rifier les logs du backend dans le terminal
4. VÃ©rifier que les modÃ¨les Ollama sont tÃ©lÃ©chargÃ©s

### Pas de streaming
1. Ouvrir la console navigateur (F12)
2. Chercher les logs avec emojis (ğŸš€ ğŸ“¡ ğŸ“¥)
3. VÃ©rifier l'URL construite
4. VÃ©rifier la rÃ©ponse rÃ©seau dans l'onglet Network

## ğŸ“Š Performance

- **Vector store** : 2994 chunks indexÃ©s
- **Embedding model** : mxbai-embed-large (334M)
- **LLM** : deepseek-v3.1:671b-cloud
- **Chunk size** : 1000 caractÃ¨res (overlap 150)
- **RÃ©ponse moyenne** : 5-15 secondes selon complexitÃ©

## ğŸ”„ Comparaison versions

| Version | Fichiers | Lignes (composant principal) | Architecture |
|---------|----------|------------------------------|--------------|
| **Ancienne** | 1 | 747 | Monolithique |
| **Nouvelle** | 35+ | 55 | Modulaire |

## ğŸ“š Documentation complÃ¨te

- **ARCHITECTURE.md** - Structure dÃ©taillÃ©e du code
- **MIGRATION.md** - Guide de migration de l'ancien code
- **DEBUG.md** - Guide de dÃ©bogage approfondi
- **IMPROVEMENTS.md** - Historique des amÃ©liorations

## ğŸ’¡ Conseils

### Pour dÃ©velopper
```bash
# Terminal 1 : Backend avec auto-reload
cd /home/se/test_ollama_rag
uvicorn server:app --reload --host 0.0.0.0 --port 8000

# Terminal 2 : Frontend avec fast refresh
cd client
npm run dev
```

### Pour tester
```bash
# Tester un endpoint spÃ©cifique
curl "http://localhost:8000/chat?question=test&k=3" -N

# VÃ©rifier le RAG
curl http://localhost:8000/rag_check
```

### Pour optimiser
- Ajuster `k` (nombre de chunks) selon le besoin
- Utiliser les filtres `doc_type` et `chapter` pour cibler
- Activer MMR pour diversifier les rÃ©sultats

## âœ¨ NouveautÃ©s de la version modulaire

1. **Architecture professionnelle** - Code organisÃ© par responsabilitÃ©
2. **TypeScript strict** - Types partout
3. **Composants rÃ©utilisables** - UI library interne
4. **TanStack Query** - Cache et optimisations
5. **Hooks customs** - Logique encapsulÃ©e
6. **Imports propres** - Alias `@/` partout
7. **Debug amÃ©liorÃ©** - Logs structurÃ©s

## ğŸ‰ PrÃªt !

Lancez le backend, lancez le frontend, et profitez de votre assistant Math RAG ! ğŸš€

Pour toute question, consultez les autres fichiers de documentation.

Bon dev ! ğŸ’»
